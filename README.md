# LearnAlgorithm

# The Master Algorithm
How the Quest for the Ultimate Learning Machine Will Remake Our World

# Prologue

# CHAPTER ONE The Machine-Learning Revolution

## Enter the learner
Computers write their own programs

## Why businesses embrace machine learning
They embrace it because they have no choice.

## Supercharging the scientific method
It follows the
same process of generating, testing, and discarding or refining hypotheses.
But while a scientist may spend his or her whole life coming up
with and testing a few hundred hypotheses, a machine-learning system
can do the same in a fraction of a second

## A billion Bill Clintons
In the future, provided voter models are accurate, elected offcials will be able to ask voters what they
want a thousand times a day and act accordingly—without having to
pester the actual esh-and-blood citizens.

## One if by land, two if by Internet
Out in cyberspace, learning algorithms man the nation’s ramparts.

## Where are we headed?
What makes this possible? How do learning algorithms work? What
can’t they currently do, and what will the next generation look like?
How will the machine-learning revolution unfold? And what opportunities
and dangers should you look out for?

# CHAPTER TWO The Master Algorithm

**All knowledge—past, present, and future—can be derived from
data by a single, universal learning algorithm**

## The argument from neuroscience
In congenitally blind people, the visual cortex can take over other
brain functions

## The argument from evolution
Life’s infinite variety is the result of a single mechanism: natural selection.

## The argument from physics
physics is unique in its simplicity. Outside physics and
engineering, the track record of mathematics is more mixed

## The argument from statistics
Bayes’ theorem is a machine that turns data into knowledge

## The argument from computer science
P and NP are the two most important classes of problems in computer
science

## Machine learners versus knowledge engineers
we should keep
their criticisms in mind as we set out on the road to the Master Algorithm

## Swan bites robot
“No matter how smart your algorithm, there are some things it just
can’t learn.”

## Is the Master Algorithm a fox or a hedgehog?
I hope the Master Algorithm
is a hedgehog, but even if it’s a fox, we can’t catch it soon enough.

Before we can discover deep truths with machine learning,
we have to discover deep truths about machine learning

## What’s at stake
Machine learning alone will not
cure cancer; cancer patients will, by sharing their data for the benet of
future patients

## A different theory of everything
A theory is a set of constraints on what the
world could be, not a complete description of it. To obtain the latter,
you have to combine the theory with data

## Candidates that don’t make the cut
Mathematicians like to say that God can disobey the
laws of physics, but even he cannot defy the laws of logic. is may be
so, but the laws of logic are for deduction; what we need is something
equivalent, but for induction

## The Five tribes of machine learning
* symbolists
* connectionists
* evolutionaries
* Bayesians
* analogizers

# CHAPTER THREE Hume’s Problem of Induction
* Rationalists
* Empiricists

## To date or not to date?
without generalization, you won’t even get o
the ground

## The "no free lunch" theorem
there’s no such thing as learning without knowledge

## Priming the knowledge pump
Suspend your disbelief and start by assuming that all
matches are good

## How to rule the world
“divide and conquer”

## Between blindness and hallucination
appropriately
enough, Probably Approximately Correct.

## Accuracy you can believe in
You can estimate the bias and variance of a learner by comparing its
predictions aer learning on random variations of the training set

## Induction is the inverse of deduction
Socrates is human.
All humans are mortal.
Terefore.... . .?...

## Learning to cure cancer
inverse deduction is a great way to discover new knowledge
in biology

## A game of twenty questions
decision tree

## The symbolists
The symbolists’ core belief is that all intelligence can be reduced to
manipulating symbols

# CHAPTER FOUR How Does Your Brain Learn?
the molecular biology of synaptic
change

## The rise and fall of the perceptron
our goal is to develop a general-purpose learning algorithm, not
to build a realistic model of the brain

## Physicist makes brain out of glass
* Spin glasses
* Boltzmann machine
* deep learning

## The most important curve in the world
Many phenomena we think of
as linear are in fact S curves, because nothing can grow without limit.

## Climbing mountains in hyperspace
Backprop

## The perceptron’s revenge
Neural networks’ rst big success was in predicting the stock market.

## A complete model of a cell
a nonlinear system

## Deeper into the brain
the top layer can be a conventional perceptron that learns
to recognize your grandmother from the high-level features provided
by the layer below it—much easier than using only the crude information
provided by a single hidden layer or than trying to backpropagate
through all the layers at once

# CHAPTER FIVE Evolution: Nature’s Learning Algorithm
Robotic Park is

in silico instead of in vivo

## Darwin’s algorithm
We can get even fancier by allowing rules for intermediate concepts
to evolve, and then chaining these rules at performance time.

## The exploration-exploitation dilemma
If you’ve found something
that works, should you just keep doing it? Or is it better to try
new things, knowing it could be a waste of time but also might lead to
a better solution?

## 程序的适者生存法则

对程序树而不是字符串进行交叉得出的一个结果就是，生成的程序可以任意大，这让学习活动变得更加灵活

## 性有何用

演化新论者非常重视，，，

机器学习主流学派已经很大程度上把他们忘记

## 先天与后天

演化论者关注的是学习架构，对他们来说，通过参数优化来对演化的架构进行微调，这是次重要的事情。

相反，联结学派更喜欢用一个简单、手工编写的结构，加上许多连接行为，然后让权值学习来完成所有工作

## 谁学的最快，谁就会赢

机器学习是地球上生命之间竞争的最新篇章，更快加速的硬件只是等式的一边，另一边是更加智能的软件

# 第六章 贝叶斯学派：在贝叶斯教堂里

## 统治世界的定理

P(原因 | 结果) = P(原因) * P(原因 | 结果) / P(结果)

贝叶斯定理之所以有用，是因为通常给定原因后，我们就会知道结果

## 所有模型都是错的，但有些却有用

如果学习算法利用贝叶斯定理，且给定原因时，假定结果相互独立，那么该学习算法被称为“朴素贝叶斯分类器”

## 从《尤金·奥涅金》到Siri

隐藏的马尔可夫模型，或者简称为HMM

## 所有东西都有关联，但不是直接关联

贝叶斯网络讲诉这样的故事：A发生了，接着它导致B的发生；

同时，C也发生了，而B和C共同引起D的发生。

为了计算特定事件的概率，我们只需将与之相关事件的概率相乘即可。

## 推理问题

马尔可夫蒙特卡洛理论（Markov chain Monte Carlo， MCMC）

贝叶斯网络中的推理不仅限于计算概率，它也包括为证据找到最可信的解释方法

单个出现可能性大的词语，一起出现时可能性就不那么大了

最重要的是，推理包括做最佳决定，引导这些决定的，不仅仅是不同结果的概率，还有相应的成本

## 掌握贝叶斯学派的方法

把假设当作可能的原因，把数据当作观察到的效果：

P（假设 | 数据） = P（假设） × P（数据 | 假设）/P（数据）

## 马尔可夫权衡证据

马尔可夫网络是一组特征以及对应的权值，特点和权值共同定义概率分布

像贝叶斯网络一样，马尔可夫网络可以通过图表来表示，但它们用无向弧而不用箭头。

两个变量被联结起来，这意味着它们会直接相互依赖，如果它们一起出现在某个体征中

## 逻辑与概率：一对不幸的组合

逻辑无法处理不完整或包含嘈杂因素的信息，这在实验生物学中较普遍，但贝叶斯网络可以沉着地处理这个问题。

贝叶斯学习能对单个数据表起作用，，，

但如果我们拥有的表格超过一个，那么贝叶斯学习就会停滞不前。

利用逻辑，我们可以轻易地写出与所有这些方面相关的规则，然后通过结合相关表格来对它们进行学习，但唯一的条件就是表格没有任何漏洞或误差。

# 第七章 类推学派：像什么就是什么

如果两个实物相似，其中的一个想法会触发另一个想法

## 完美另一半

最近邻法是人类有史以来发明的最简单、最快速的学习算法

最近邻算法是史上第一个能够利用不限数量的数据来掌握任意复杂概念的算法

## 维数灾难

随着维数的上升，用于确定概念边界的训练样本的数量也会呈指数上升

我们能做的第一件事就是摆脱不相关维度。

决策树会自行做好这一点，方法发就是计算每种属性的信息增益，然后只使用最能提供信息的属性。

对于最近邻算法来说，我们可以完成类似的事情，方法就是首先丢弃所有那些信息增益低于阙值的属性，然后只在简化的空间中测量相似性

要处理弱相关的属性，一个选择就是掌握属性权值

## 空中蛇灾

支持向量机（Support vector machines， SVM）

表面上看，支持向量机看起来很像加权k最近邻算法：

正类别与负类别之间的边界由一组例子、其权值加上相似性测度来确定。

测试实例会归入正类别，条件是从平均水平上看，它看起来更像正面例子而不是负面例子。

平均数会被加权，而支持向量机只会记住那些用于确定边界的关键例子。

为了学习支持向量机，我满需要选择向量和它们的权值。

相似性度量，在支持向量机领地被称为“核心程序”，通常被归为先验性。

支持向量机无法幸免于维数灾难，但它们最能抵抗这个灾难

## 爬上梯子

两个东西如果在一方面意见一致，那么它们就是相似的。

如果它们在一些方面意见一致，可能在其他方面也会意见一致，这就是类比的本质。

它还表明了类比推理中的两大子问题：弄明白两个实物的相似度，确定由它们的相似度还能推导出什么。

在任何类比学习算法中，最重要的问题就是如何度量相似性。

类似学者最棒的几圈在于跨越问题域来进行学习。

## 起床啦

规则可以类推地进行匹配，所以它们不会再那么脆弱。

实例可以选择空间中不同区域的不同特点，然后比最近邻算法更能与维数灾难相抗衡，而最近邻算法只能到处选择相同的特征。

# 第八章 无师自通

一磅的胶状物如何变成意识的所在处

## 物以类聚，人以群分

我们需要一种能够自发将所有相似物体或者同一物体的不同图片集中起来的算法。这就是聚类问题，这在机器学习当中也是人家研究最多的主题之一。

对每个集群来说，我们先计算其成员的平均特性，然后将其当作新的典范。

k均值算法（k-means algorithm）

有一个更大的问题，那就是k均值算法只有在集群易于区分的情况下才能起作用

期望最大化演算法（Expection Maximization，EM算法）

## 发现数据的形状

机器学习算法称该过程为维数约简，因为该过程将大量的可见维度（像素）简化成几个隐形维度（表情、面部特征）。

维数约简对于应对大数据（像每秒钟通过你的知觉而进入的数据）来说很关键。

主要成分分析（principle-component analysis，PCA）

对于非线性降维算法来说，最受欢迎的算法-等距映射算法

从了解视频中的动作到探测言语中的情绪，等距映射有惊人的能力，可以对准复杂数据中最重要的维度

## 拥护享乐主义的机器人

强化学习通过估算每种状态的价值来做到这一点，从该状态开始你所期望得到的全部奖励，然后选择能将奖励最大化的行为。

用强化学习来将之前见过的一个监督式学习算法（例如，多层感知器）包括进来。

这时神经网络的工作就是预测状态的价值，而反向传播的误差信号就是预测价值与所观察到的价值之间的差别。

## 熟能生巧

组块是一个来自感知与记忆心理学的概念。

我们以组块的形式来感知并记住东西，而在任意给定的时间内（根据乔治·米勒的经典论文的结论为7±2），我们只能通过短暂记忆来记住这么多组块。jb

A/B测试，，，数据可以根据需要集中起来，从营销到对外援助。它还可以概括成一次性尝试许多变化组合，而不会忽略什么样的改变会带来什么（或者失去什么）

## 学会关联

网络中的节点会互相作用；你对这个节点做了什么，会影响到其他节点，最后还会返回来对你进行影响。






